== Using the Kafka API to exchange data between two microservices

In this part of the workshop we're going to use the Kafka to propagate data from one microservice to another.

You'll learn the following things:

* Sending messages to Kafka
* Receiving messages from Kafka

We have prepared two simple microservices, one which receives "customer orders" via a REST API and another one which should get hold of any order in order to produce "invoices".
The services are based on https://thorntail.io/[Thorntail], an approach for building microservices using Java EE and MicroProfile.

=== Cloning the Example Services

Begin by creating a fork of the https://github.com/debezium/microservices-lab[microservices-lab] GitHub repository under your own GitHub user.
Then clone your forked repository to your _local machine_ and check out the "module2-start" branch:

[source, sh]
git clone git@github.com:<your user>/microservices-lab.git
cd microservices-lab
git checkout -b module2 origin/module2-start

Import the two projects _ticket-msa/order_ and _ticket-msa/invoice_ into your preferred IDE.

=== Setting Up the Producer Application

The producer application receives orders for given events via a REST API.
It already has been prepared to persist the orders in a MySQL database.

Now it should also emit a message for each placed order to Kafka.
For doing so, we're going to use the https://github.com/aerogear/kafka-cdi[Kafka CDI] portable extension,
which allows to obtain a message producer via dependency injection.
Alternatively, the plain Kafka API could be used as well.

Emitting messages to Kafka will allow other services, e.g. an invoicing service to react to each created order.

In the following, update _ticket-msa/order/src/main/java/io/debezium/examples/ticketmsa/order/OrderService.java_ like so:

* Add the `@KafkaConfig` annotation to the class, which will inject the required Kafka configuration:

    ```
    @KafkaConfig(bootstrapServers = "#{KAFKA_SERVICE_HOST}:#{KAFKA_SERVICE_PORT}")
    public class OrderService {
        ...
    }
    ```

* Add a field for injecting the topic name to send to (The MicroProfile Config API is used for this):

    ```
    @Inject
    @ConfigProperty(name="order.topic.name", defaultValue="orders")
    private String topicName;
    ```

* Add a field for injecting the Kafka producer:

    ```
    @Producer
    private SimpleKafkaProducer<Integer, JsonObject> kafka;
    ```

* Adjust the `addOrder()` method so it sends a message to Kafka:

    ```
    public Order addOrder(Order order) {
        order = entityManager.merge(order);
        kafka.send(topicName, order.getId(), order.toJson());
        return order;
    }
    ```

Eventually, the class should look like https://github.com/debezium/microservices-lab/blob/master/ticket-msa/order/src/main/java/io/debezium/examples/ticketmsa/order/OrderService.java[this].

Commit your change and push it to your GitHub fork:

[source, sh]
git commit -a -m "Implementing order service"
git push origin module2

Now change to the VM where you have started OpenShift and start a MySQL instance by running:

[source, sh]
$ oc new-app --name=mysql debezium/example-mysql:0.8
$ oc set env dc/mysql MYSQL_ROOT_PASSWORD=debezium MYSQL_USER=mysqluser MYSQL_PASSWORD=mysqlpw

Then build and start the application.
This uses the https://github.com/fabric8/s2i-java[Fabric8 S2I] ("source to image") builder image.
It will fetch the application's source code from the given location, build it using Maven and then run the Thorntail uber JAR:

[source,sh]
----
$ oc new-app --name=order-msa fabric8/s2i-java:latest~https://github.com/<your fork>/microservices-lab#module2 \
    --context-dir=ticket-msa/order \
    -e MYSQL_DATABASE=inventory \
    -e AB_PROMETHEUS_OFF=true \
    -e KAFKA_SERVICE_HOST=my-cluster-kafka-bootstrap \
    -e KAFKA_SERVICE_PORT=9092 \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true \
    -e ORDER_TOPIC_NAME=myorders
----

If you couldn't apply the code changes yourself, you can also use the completed version from the upstream repo at https://github.com/debezium/microservices-lab.

Next we need to expose port 8080 for the service:

[source,sh]
$ oc patch service order-msa -p '{ "spec" : { "ports" : [{ "name" : "8080-tcp", "port" : 8080, "protocol" : "TCP", "targetPort" : 8080 }] } } }'
$ oc expose svc order-msa

It's also a good idea to convert the application's build into an "incremental build",
which will avoid refetching all dependencies from remote Maven repostories after the first build:

[source,sh]
$ oc patch bc/order-msa -p '{"spec":{"strategy":{"sourceStrategy":{"incremental":true}}}}'

Once the application and its database are running
(use `oc get pods` to verify, alternatively, examine the "order-msa" application's status in the OpenShift web console),
you can place "orders" by submitting requests like this to the application's REST API:

[source]
----
$ oc exec -c kafka -i my-cluster-kafka-0 -- curl -X POST -s -w "\n" \
    -H "Accept:application/json" \
    -H "Content-Type:application/json" \
    http://order-msa:8080/rest/orders -d @- <<'EOF'
{
    "firstName":"John",
    "lastName":"Doe",
    "email":"john.doe@example.com",
    "price":1000
}
EOF
----

The reply should contain the id generated for the order.
We also can examine that it has been persisted in the database.
To do so, open a shell on the database's pod and log into MySQL:

[source,sh]
----
$ oc rsh $(oc get pods -o name -l app=mysql)
$ mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory
# In the MySQL shell:
# select * from MSA_ORDER;
# exit
exit
----

At the same time, a corresponding message should have been produced to Kafka.
Let's take a look at the topic using the console consumer coming with Kafka:

[source,sh]
----
$ oc exec -c kafka -it my-cluster-kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
   --bootstrap-server my-cluster-kafka-bootstrap:9092 \
   --from-beginning \
   --property print.key=true \
   --topic myorders
----

Once done, hit Ctrl + C to exit the console consumer.

=== Setting Up the Consumer Application

Once order messages arrive in the "myorders" topic, it's time to set up another service, invoice,
which will receive the messages using Kafka's consumer API.

Change to your local checkout again, and edit the file _ticket-msa/invoice/src/main/java/io/debezium/examples/ticketmsa/invoice/InvoiceService.java_:

* Add the `@KafkaConfig` annotation to the class, which will inject the required Kafka configuration:

    ```
    @KafkaConfig(bootstrapServers = "#{KAFKA_SERVICE_HOST}:#{KAFKA_SERVICE_PORT}")
    public class OrderService {
        ...
    }
    ```

* Add an event handler method which will be invoked by the Kafka CDI extension for each message received on the "myorders" topic (the actual value is injected via an environment variable):

    ```
    @Consumer(topics = "#{ORDER_TOPIC_NAME}", groupId = "InvoiceService")
    public void orderArrived(final String order) {
        LOGGER.info("Order event '{}' arrived", order);
    }
    ```

Eventually, the file should look like https://github.com/debezium/microservices-lab/blob/master/ticket-msa/invoice/src/main/java/io/debezium/examples/ticketmsa/invoice/InvoiceService.java[this].

Commit the change and push it to your GitHub fork:

[source, sh]
git commit -a -m "Implementing Kafka consumer"
git push origin module2

Switch to the console running OpenShift.
The "invoice" app can be run similar to the one above, only the "--context-dir" is different:
The steps are the same as above, only that we're building the invoice application this time:

[source,sh]
----
$ oc new-app --name=invoice-msa fabric8/s2i-java:latest~https://github.com/<your fork>/microservices-lab#module2 \
    --context-dir=ticket-msa/invoice \
    -e AB_PROMETHEUS_OFF=true \
    -e KAFKA_SERVICE_HOST=my-cluster-kafka-bootstrap \
    -e KAFKA_SERVICE_PORT=9092 \
    -e JAVA_OPTIONS=-Djava.net.preferIPv4Stack=true \
    -e ORDER_TOPIC_NAME=myorders

$ oc patch service invoice-msa -p '{ "spec" : { "ports" : [{ "name" : "8080-tcp", "port" : 8080, "protocol" : "TCP", "targetPort" : 8080 }] } } }'

$ oc expose svc invoice-msa

$ oc patch bc/invoice-msa -p '{"spec":{"strategy":{"sourceStrategy":{"incremental":true}}}}'
----

Once the example application has started, it will simply logs each order message it receives.
Send another POST request to the order service as shown above.
Then take a look at the logs of the invoice application:

[source,sh]
----
$ oc logs $(oc get pods -o name -l app=invoice-msa)
----

You should see messages like this:

[source]
----
2018-10-25 07:17:08,412 INFO  [io.debezium.examples.ticketmsa.invoice.InvoiceService] (EE-ManagedExecutorService-default-Thread-1) Order event '{"id":7,"firstName":"John","lastName":"Doe","email":"john.doe@example.com","price":1000}' arrived
----

In this part of the lab you've learned how to propagate data between two microservices using Kafka.

There's one potential problem, though: the "order" application writes data to its database _and_ Kafka at the same time.
As these two resources are not modified within a single global transaction, it might happen that inconsistencies occur e.g. when the change is applied to the database but the write to Kafka failed for some reason.
In the following part we'll introduce an alternative approach which avoids these issues by tracking changes in the database in order to write them into Kafka.
This is known as "change data capture".

Once done with this part of the workshop, delete the two applications like so:

[source,sh]
$ oc delete all -l app=invoice-msa
$ oc delete all -l app=order-msa
