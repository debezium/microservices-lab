== Using the Kafka API to exchange data between two microservices
ifdef::env-github,env-browser[:outfilesuffix: .adoc]

In this part of the workshop we're going to use the Kafka to propagate data from one microservice to another.

You'll learn the following things:

* Sending messages to Kafka
* Receiving messages from Kafka

We have prepared two simple microservices, one which receives "customer orders" via a REST API and another one which should get hold of any order in order to produce "invoices".
The services are based on http://quarkus.io/[Quarkus], a "Kubernetes Native Java stack tailored for GraalVM & OpenJDK HotSpot, crafted from the best of breed Java libraries and standards".

=== The Order Service

The order service receives purchase orders (for event tickets) via a REST API.
It persists these orders in a Postgres database.

Now it should also emit a message for each placed order to Kafka.
For doing so, we're going to use the https://github.com/smallrye/smallrye-reactive-messaging[SmallRye Reactive Messaging API], which is an implementation of the https://github.com/eclipse/microprofile-reactive-messaging[MicroProfile Reactive Messaging API specification].
In a nutshell, it is CDI extension to build data streaming applications based on top of Apache Kafka, MQTT, AMQP and others.
Alternatively, the plain Kafka API could be used as well.

Emitting messages to Kafka will allow other services, e.g. an invoicing service to react to each created order.

Using this API, the following is all that it's needed for implementing the functionality described above in the https://github.com/debezium/microservices-lab/blob/master/ticket-msa/order/src/main/java/io/debezium/examples/ticketmsa/order/OrderService.java[OrderService] class:

[source,java]
----
@Path("/orders")
@ApplicationScoped
public class OrderService {

    @Inject
    @Stream("orders")
    private Emitter<KafkaMessage<Integer, String>> orderStream;

    @PersistenceContext
    private EntityManager entityManager;

    @POST
    @Transactional
    @Produces("application/json")
    @Consumes("application/json")
    public Order addOrder(Order order) {
        order = entityManager.merge(order);
        orderStream.send(KafkaMessage.of(order.getId(), order.toJson().toString()));
        return order;
    }
}
----

This code:

* Provides a REST endpoint for placing new purchase orders via JAX-RS
* Uses the Java Persistence API (JPA) to store the new orders in the Postgres database
* Emits a Kafka message representing the new order

The `Emitter` instance `orderStream` is particularly interesting:
it is provided by SmallRye and can be used to feed a stream from an imperative piece of code,
such as our JAX-RS endpoint.
The `@Stream` references a logical event stream named "orders",
which is defined in the application.properties file:

[source]
----
smallrye.messaging.sink.orders.type=io.smallrye.reactive.messaging.kafka.Kafka
smallrye.messaging.sink.orders.topic=OrderEvents
smallrye.messaging.sink.orders.bootstrap.servers=production-ready-kafka-bootstrap:9092
smallrye.messaging.sink.orders.key.serializer=org.apache.kafka.common.serialization.IntegerSerializer
smallrye.messaging.sink.orders.value.serializer=org.apache.kafka.common.serialization.StringSerializer
----

This configuration defines that this stream should be bound to a Kafka topic named "OrderEvents",
using the given Kafka bootstrap server (the entry point into the cluster which allows the client to identify all the nodes in the cluster) and the integer and string serializers for message key and value, respectively.

Let's deploy the order service application now.
Begin by setting up an instance of Postgres as the database for the service.
For this, we need a dedicated service account which will be used to run the database container
(that's required due to the way the Postgres image handles file permissions internally):

Go to your SSH session where you've logged into OpenShift and run the following:

[source]
$ oc create sa debezium -n amq-streams
$ oc adm policy add-scc-to-user anyuid system:serviceaccount:amq-streams:debezium

Then we can set up a new instance of Postgres:

[source,sh]
----
$ oc new-app --name=orderdb debezium/example-postgres:0.9 \
    -e POSTGRES_USER=postgres \
    -e POSTGRES_PASSWORD=postgres

$ oc patch dc/orderdb --type merge -p '{ "spec" : { "template" : { "spec" : { "serviceAccountName" : "debezium" } } } }'
----

Then build and start the application.
We're going to use https://docs.okd.io/latest/using_images/s2i_images/index.html[OpenShift's S2I] ("source to image") process for this.
The S2I process will fetch the application's source code from the given GitHub location, build it using Maven and then run the application produced by the Quarkus Maven plug-in.
We're using the https://github.com/fabric8/s2i-java[Fabric8 S2I] builder image,
which can be used for building and running Java applications on OpenShift.

[source,sh]
----
$ oc new-app --name=order fabric8/s2i-java:latest~https://github.com/debezium/microservices-lab \
    --context-dir=ticket-msa/order \
    -e AB_PROMETHEUS_OFF=true
----

We also must expose a "service" for the application, allowing it to receive HTTP requests
Uti:
[source,sh]
----
$ oc expose svc order

route.route.openshift.io/order exposed
----

Use `oc get pods` to verify that the build has completed and the application is running:

[source,sh]
----
$ oc get pods

NAME                                          READY     STATUS      RESTARTS   AGE
order-1-build                             0/1       Completed   0          58s
order-1-hwdp8                             1/1       Running     0          5s
...
----

You also can take a look at the logs of the build:

[source,sh]
----
$ oc logs order-1-build

...
Pushed 30/31 layers, 97% complete
Pushed 31/31 layers, 100% complete
Push successful
----

After a while, the logs of the application itself should say that Quarkus has started:

[source,sh]
----
$ oc logs $(oc get pods -o name -l app=order)

...
2019-04-26 07:17:12,455 INFO  [io.quarkus] (main) Quarkus 0.13.3 started in 2.610s. Listening on: http://[::]:8080
2019-04-26 07:17:12,455 INFO  [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, jdbc-postgresql, narayana-jta, resteasy, resteasy-jsonb, smallrye-reactive-messaging, smallrye-reactive-messaging-kafka, vertx]
----

Alternatively, examine the "order" application's status in the OpenShift web console.

Now you can place "orders" by submitting requests like this to the application's REST API.
For that, open a second shell session to the workstation node of the cluster
and launch an instance of the tooling pod as done before in the previous module:

[source, sh]
$ oc run tooling -it --image=debezium/tooling --restart=Never

In the container submit the following request:

[source,sh]
----
$ http POST http://order:8080/rest/orders \
    firstName=John \
    lastName=Doe \
    email=john.doe@example.com \
    price=1000
----

The reply should contain the id generated for the order.
We also can examine that it has been persisted in the database.
To do so, launch pgcli (still within the tooling pod):

[source,sh]
----
$ pgcli postgresql://postgres:postgres@orderdb:5432/postgres
# In the PG shell:
# select * from public.msa_order;
# exit
----

At the same time, a corresponding message should have been produced to Kafka.
Let's take a look at the topic using kafkacat:

[source,sh]
----
$ kafkacat -b production-ready-kafka-bootstrap -t OrderEvents -o beginning -f 'offset: %o, key: %k, value: %s\n'
----

Once done, hit Ctrl + C to exit kafkacat.

=== Setting Up the Consumer Application

Once order messages arrive in the "OrderEvents" topic, it's time to set up another service, invoice,
which will receive the messages using Kafka's consumer API.

Again we're using the MicroProfile Reactive Messaging API, in this case for consuming messages from the "OrderEvents" topic.
This is how the https://github.com/debezium/microservices-lab/blob/master/ticket-msa/invoice/src/main/java/io/debezium/examples/ticketmsa/invoice/InvoiceService.java[InvoiceService] class looks like:

[source,java]
----
@ApplicationScoped
public class InvoiceService {

    private static final Logger LOGGER = LoggerFactory.getLogger(InvoiceService.class);

    @Incoming("orders")
    public void orderArrived(final String order) {
        LOGGER.info("Order event '{}' arrived", order);
    }
}
----

This time the `@Incoming` method is used to mark a method which should be invoked whenever a new event arrives on the "orders" stream.
As in the order service, this stream is configured in application.properties,
only that it is a source instead of a sink stream this time:

[source]
----
smallrye.messaging.source.orders.type=io.smallrye.reactive.messaging.kafka.Kafka
smallrye.messaging.source.orders.topic=OrderEvents
smallrye.messaging.source.orders.bootstrap.servers=production-ready-kafka-bootstrap:9092
smallrye.messaging.source.orders.key.deserializer=org.apache.kafka.common.serialization.IntegerDeserializer
smallrye.messaging.source.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
smallrye.messaging.source.orders.group.id=InvoiceService
----

Now switch to the console where you're running the `oc` commands.
The "invoice" app can be run similar to the one above, only the "--context-dir" is different:

[source,sh]
----
$ oc new-app --name=invoice fabric8/s2i-java:latest~https://github.com/debezium/microservices-lab \
    --context-dir=ticket-msa/invoice \
    -e AB_PROMETHEUS_OFF=true
----

Once the example application has started (verify similarly to the order service above), it will simply logs each order message it receives.
Go to the tooling pod and send another POST request to the order service as shown above.
Then take a look at the logs of the invoice application:

[source,sh]
----
$ oc logs $(oc get pods -o name -l app=invoice)
----

You should see messages like this:

[source]
----
2019-04-26 07:59:10,828 INFO  [io.deb.exa.tic.inv.InvoiceService] (vert.x-eventloop-thread-0) Order event '{"id":8,"firstName":"John","lastName":"Doe","email":"john.doe@example.com","price":1000}' arrived
----

=== Summary

In this part of the lab you've learned how to propagate data between two microservices using Kafka.

There's one potential problem, though: the "order" application writes data to its database _and_ Kafka at the same time.
As these two resources are not modified within a single global transaction, it might happen that inconsistencies occur e.g. when the change is applied to the database but the write to Kafka failed for some reason.
In <<module-03#,module 3>> we'll introduce an alternative approach which avoids these issues by tracking changes in the database in order to write them into Kafka.
This is known as "change data capture".

Once done with this part of the workshop, delete the two applications and the database like so:

[source,sh]
$ oc delete all -l app=invoice
$ oc delete all -l app=order
$ oc delete all -l app=orderdb
