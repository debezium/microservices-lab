== Deploying an Apache Kafka Cluster with AMQ Streams
ifdef::env-github,env-browser[:outfilesuffix: .adoc]
:imagesdir: ./images

Let's get started by deploying an Apache Kafka cluster on OpenShift.
For this, we will use AMQ Streams, an open-source project that simplifies the process of deploying and managing Apache Kafka clusters on Kubernetes and OpenShift.

=== How AMQ Streams works

You can run an Apache Kafka cluster on Kubernetes, and by extension, on OpenShift, in a variety of ways, not all being equal in terms of ease of use and maintenance.

For example, you can deploy the cluster manually as a stateful set.
While this can get you past the initial hurdle of starting the cluster, soon you have to start performing more complex tasks such as changing cluster topology, modifying configuration, or administering topics.
These tasks typically require direct access to the cluster nodes and can easily become cumbersome.

==== Kubernetes Operators ====

AMQ Streams simplifies these tasks by using a declarative approach to cluster and topic management, implemented as https://coreos.com/operators/[Kubernetes Operators].
A Kubernetes Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.
It builds upon the basic Kubernetes resource and operator concepts but includes domain or application-specific knowledge to automate common tasks.

Instead of relying on direct deployment and management of ZooKeeper and Kafka clusters, Strimzi consists of a couple of these domain-specific operators that monitor the state of the cluster, making adjustments in accordance to a desired state read from dedicated configuration resources.

For creating an Apache Kafka cluster, for instance, you need to create a resource of type `Kafka` that describes the properties of the cluster, and the *_cluster operator_* will deploy the cluster for you.
If you need to change the state of the cluster, for example for changing properties or for adding new instances, all you have to do is to modify the resource and the changes will be rolled out accordingly.

Topic management works in a similar fashion: for creating and modifying topics, you only need to create and edit a set of resources of type `KafkaTopic` and the *_topic operator_* will do the work for you.

You will do all this as part of the first lab.

=== Logging In Into OpenShift

Once you have SSH'ed into the workstation node, log into to OpenShift as the `admin` user with the password supplied by the instructor.

[source, sh]
$ oc login -u admin master00.example.com

https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[Namespaces] (or "projects" in OpenShift terms) are a concept of Kubernetes for organizing and dividing the resources of a cluster between multiple users.
There should already be a project named "amq-streams" (verify by running `oc get projects`).
In order to work within this project, run:

[source, sh]
----
$ oc project amq-streams

Now using project "amq-streams" on server "https://master00.example.com:443".
----

If the project doesn't exist yet, you can created it like so:

[source, sh]
oc new-project amq-streams

The project should be empty:

[source, sh]
$ oc get pods
No resources found.

=== Installing AMQ Streams and Setting Up a Kafka Cluster

The AMQ Streams installation files have already been downloaded to the workstation node.
You can find them in the _kafka_ directory:

[source, sh]
$ cd kafka

Prior to installing the cluster operator, we will need to configure the namespaces it operates with. We will do this by modifying the _\*RoleBinding\*.yaml_ files to point to the newly created project amq-streams.
You can do this by simply editing all files via sed.

[source, sh]
$ sed -i 's/namespace: .*/namespace: amq-streams/' install/cluster-operator/*RoleBinding*.yaml

Once the configuration files are changed, you can install the cluster operator:

[source, sh]
$ oc apply -f install/cluster-operator -n amq-streams

You should see a few resources being created:

[source, sh]
serviceaccount/strimzi-cluster-operator created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced configured
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created
clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global configured
clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator configured
clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker configured
clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation configured
clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator configured
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created
clusterrole.rbac.authorization.k8s.io/strimzi-topic-operator configured
rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-topic-operator-delegation created
customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io configured
customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io configured
customresourcedefinition.apiextensions.k8s.io/kafkaconnects2is.kafka.strimzi.io configured
customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io configured
customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io configured
customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io configured
deployment.extensions/strimzi-cluster-operator created

The service account `strimzi-cluster-operator` is granted permission to access various resources in the project.
This allows it to read the resources containing the cluster configuration that we will create later in the process.

Now, make sure that the cluster operator is deployed.

[source,sh]
$ oc get pods

The command output should be similar to:

[source,sh]
NAME                                          READY     STATUS    RESTARTS   AGE
strimzi-cluster-operator-2044197322-lzrvr   1/1       Running   0          3m

You also can log into the OpenShift web console to do the same.
Go to https://master00-<GUID>.generic.opentlc.com/ (using the same credentials),
accept all warnings about certificate issues.
Choose the "amq-streams" project from the menu on the right.
If you see a warning "An error occurred getting metrics", just click on "Don't Show Me Again",
as it's not relevant for this lab.
You should see a single deployed application, "strimzi-cluster-operator".
All the applications you'll deploy in the following will show up automatically, too.

The next step is to deploy an actual Kafka Cluster.
This is done by providing a custom resource definition (CRD) which describes the desired cluster
(e.g. number of nodes).
The AMQ Streams cluster operator will process the CRD and create a Kafka cluster matching the description.

We'd like to have a cluster which has the following characteristics:

* Making sure that you deploy at least 3 Kafka brokers for scaling and HA
* Making sure that topics are replicated on at least two nodes
* Making sure that your Zookeeper ensemble has at least 3 nodes
* Making sure that the data of your Kafka cluster is persistent

That's achieved by deploying the following Kafka CRD:

[source,yaml]
--
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: production-ready
spec:
  kafka:
    replicas: 3
    listeners:
      plain: {}
      tls: {}
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
    storage:
      type: persistent-claim
      size: 3Gi
      deleteClaim: false
  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 1Gi
      deleteClaim: false
  entityOperator:
    topicOperator: {}
    userOperator: {}
--

Letâ€™s deploy this new resource:

[source, sh]
$ oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/production-ready.yaml

Let's take a look at the resource we've created:

[source]
----
$ oc describe kafka production-ready

Name:         production-ready
Namespace:    amq-streams
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"Kafka","metadata":{"annotations":{},"name":"production-ready","namespace":"amq-streams"},"spec":{"ent...
API Version:  kafka.strimzi.io/v1alpha1
Kind:         Kafka
Metadata:
  Creation Timestamp:  2019-04-16T10:21:45Z
  Generation:          1
  Resource Version:    264361
  Self Link:           /apis/kafka.strimzi.io/v1alpha1/namespaces/amq-streams/kafkas/production-ready
  UID:                 6fca94ca-6031-11e9-a9e8-2cabcdef0010
Spec:
  Entity Operator:
    Topic Operator:
    User Operator:
  Kafka:
    Config:
      Offsets . Topic . Replication . Factor:            3
      Transaction . State . Log . Min . Isr:             2
      Transaction . State . Log . Replication . Factor:  3
    Listeners:
      Plain:
      Tls:
    Replicas:  3
    Storage:
      Delete Claim:  false
      Size:          3Gi
      Type:          persistent-claim
  Zookeeper:
    Replicas:  3
    Storage:
      Delete Claim:  false
      Size:          1Gi
      Type:          persistent-claim
Events:              <none>
----

Note how for instance the number of Kafka and ZooKeeper nodes is controlled using the `Replicas` parameters.

Visualize the running pods:

[source,sh]
$ oc get pods -w

Wait until all pods have spun up and are in `Running` status:

[source,sh]
----
$ oc get pods -w

NAME                                          READY     STATUS    RESTARTS   AGE
production-ready-entity-operator-67b9b54b88-zfsp5   3/3       Running   0          24s
production-ready-kafka-0                            2/2       Running   0          1m
production-ready-kafka-1                            2/2       Running   0          1m
production-ready-kafka-2                            2/2       Running   0          1m
production-ready-zookeeper-0                        2/2       Running   0          1m
production-ready-zookeeper-1                        2/2       Running   0          1m
production-ready-zookeeper-2                        2/2       Running   0          1m
strimzi-cluster-operator-696658566-9bttk            1/1       Running   0          46m
----

In addition to the `cluster operator` created previously, notice a few more deployments:

* the `entity operator` is now deployed as well - you can deploy it independently, but the Strimzi template deploys it out of the box; it is used to manage topics and/or users of Kafka
* three ZooKeeper node
* three Kafka brokers

Also, notice that the ZooKeeper ensemble and the Kafka cluster are deployed as stateful sets.

=== Testing the Deployment

Now, let's quickly test that the deployed Kafka cluster works.
Execute the following:

[source, sh]
$ oc run producer -it --image=debezium/tooling --restart=Never

This starts a container with some useful tooling for dealing with Apache Kafka and its topics,
e.g. kafkacat for sending and consuming messages, httpie for invoking REST APIs and some more.

Let's send some messages using kafkacat:

----
echo "mykey:test" | kafkacat -b production-ready-kafka-bootstrap -t test-topic -Z -K:
echo "mykey:test2" | kafkacat -b production-ready-kafka-bootstrap -t test-topic -Z -K:
----

Open another SSH session on your workstation to the cluster and fire up another tooling pod:

[source, sh]
$ oc run consumer -it --image=debezium/tooling --restart=Never

Start kafkacat in consumer mode for receiving the messages sent above:

----
kafkacat -b production-ready-kafka-bootstrap -t test-topic -o beginning -f 'offset: %o, key: %k, value: %s\n'
----

Once the consumer is started, you should see the previously sent messages in the output.
Reverting to the terminal where we started the console producer and sending any new messages there will result in those messages being displayed in the consumer terminal.

Of course also all the tools coming with Apache Kafka can be used, for instance in order to list all the available topics:

[source]
----
$ oc exec -it production-ready-kafka-0 -c kafka -- bin/kafka-topics.sh --zookeeper localhost:2181 --list
----

Finally, exit from the terminal of both containers:
----
exit
----

Also delete the corresponding pods:

[source]
----
$ oc delete pod producer
$ oc delete pod consumer
----

Now that your Kafka cluster is running and ready to go, let's continue with <<module-02#,module 2>> and build some applications!
